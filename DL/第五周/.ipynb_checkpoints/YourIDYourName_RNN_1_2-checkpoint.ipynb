{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, LSTM, Bidirectional, Dropout\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 数据集配置\n",
    "MAX_FEATURES = 10000   # 使用最常见的前 10,000 个单词\n",
    "MAXLEN_DEFAULT = 200   # 默认序列长度\n",
    "\n",
    "# 为了训练速度，我们只使用部分样本\n",
    "N_TRAIN_SAMPLES = 20000\n",
    "N_TEST_SAMPLES  = 10000\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 嵌入层实验用的超参数候选（实验 1-1）\n",
    "MAXLEN_LIST = [100, 200, 300]     # 不同的样本长度\n",
    "EMBED_DIM_LIST = [8, 16, 32]      # 不同的词向量维度\n",
    "EPOCHS_EMBED = 5                  # 每个组合训练轮数\n",
    "\n",
    "# RNN 实验（实验 2）超参数\n",
    "MAXLEN_RNN = 200\n",
    "EMBED_DIM_RNN = 32\n",
    "EPOCHS_RNN = 5                    # LSTM / 堆叠 LSTM / BiLSTM 的训练轮数\n",
    "\n",
    "print(\"MAX_FEATURES:\", MAX_FEATURES)\n",
    "print(\"N_TRAIN_SAMPLES:\", N_TRAIN_SAMPLES, \"N_TEST_SAMPLES:\", N_TEST_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words=MAX_FEATURES: 只保留词频最高的 MAX_FEATURES 个单词\n",
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = imdb.load_data(num_words=MAX_FEATURES)\n",
    "\n",
    "print(\"原始训练集大小:\", len(x_train_raw))\n",
    "print(\"原始测试集大小:\", len(x_test_raw))\n",
    "\n",
    "# 为了节省时间，截取前 N_TRAIN_SAMPLES / N_TEST_SAMPLES 条样本\n",
    "x_train_raw = x_train_raw[:N_TRAIN_SAMPLES]\n",
    "y_train_raw = y_train_raw[:N_TRAIN_SAMPLES]\n",
    "x_test_raw  = x_test_raw[:N_TEST_SAMPLES]\n",
    "y_test_raw  = y_test_raw[:N_TEST_SAMPLES]\n",
    "\n",
    "print(\"使用的训练集大小:\", len(x_train_raw))\n",
    "print(\"使用的测试集大小:\", len(x_test_raw))\n",
    "print(\"示例原始序列（前 1 条）:\", x_train_raw[0][:20], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title_prefix=\"\"):\n",
    "    \"\"\"绘制 accuracy / loss 曲线。\"\"\"\n",
    "    acc = history.history.get(\"acc\") or history.history.get(\"accuracy\")\n",
    "    val_acc = history.history.get(\"val_acc\") or history.history.get(\"val_accuracy\")\n",
    "    loss = history.history.get(\"loss\")\n",
    "    val_loss = history.history.get(\"val_loss\")\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "    plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "    plt.title(f\"{title_prefix} Training and validation accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    plt.title(f\"{title_prefix} Training and validation loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词嵌入进行电影评论分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_model(maxlen, embedding_dim):\n",
    "    \"\"\"构建一个 Embedding + Flatten + Dense 的简单网络。\"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, embedding_dim, input_length=maxlen))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_embed = []\n",
    "\n",
    "for maxlen in MAXLEN_LIST:\n",
    "    # 对当前 maxlen 重新进行 padding\n",
    "    x_train = sequence.pad_sequences(x_train_raw, maxlen=maxlen)\n",
    "    x_test  = sequence.pad_sequences(x_test_raw,  maxlen=maxlen)\n",
    "    print(\"\\n=== 当前样本长度 maxlen =\", maxlen, \"===\")\n",
    "    print(\"x_train shape:\", x_train.shape, \"x_test shape:\", x_test.shape)\n",
    "\n",
    "    for embedding_dim in EMBED_DIM_LIST:\n",
    "        print(f\"\\n--> 训练模型：maxlen={maxlen}, embedding_dim={embedding_dim}\")\n",
    "        model = build_embedding_model(maxlen=maxlen, embedding_dim=embedding_dim)\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(\n",
    "            x_train, y_train_raw,\n",
    "            epochs=EPOCHS_EMBED,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=0.2,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test_raw, verbose=0)\n",
    "        print(f\"[结果] maxlen={maxlen}, embedding_dim={embedding_dim}, test_acc={test_acc:.4f}\")\n",
    "\n",
    "        results_embed.append({\n",
    "            \"maxlen\": maxlen,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"test_loss\": float(test_loss),\n",
    "            \"test_acc\": float(test_acc)\n",
    "        })\n",
    "\n",
    "        plot_history(history, title_prefix=f\"[Embedding] len={maxlen}, dim={embedding_dim}\")\n",
    "\n",
    "import json\n",
    "print(\"\\n=== 不同参数组合的测试集结果汇总 ===\")\n",
    "print(json.dumps(results_embed, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot（词袋）/TF-IDF/词嵌入的对比  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sequences_to_texts(sequences):\n",
    "    \"\"\"将整数序列转换为以空格分隔的 token 字符串（便于 sklearn 使用）。\"\"\"\n",
    "    return [\" \".join(str(i) for i in seq) for seq in sequences]\n",
    "\n",
    "train_texts = sequences_to_texts(x_train_raw)\n",
    "test_texts  = sequences_to_texts(x_test_raw)\n",
    "\n",
    "print(\"示例文本：\", train_texts[0][:200], \"...\")\n",
    "\n",
    "print(\"\\n=== 词袋模型（One-Hot / 计数） ===\")\n",
    "vectorizer_bow = CountVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"   # 保留所有 token（包括单个数字）\n",
    ")\n",
    "X_train_bow = vectorizer_bow.fit_transform(train_texts)\n",
    "X_test_bow  = vectorizer_bow.transform(test_texts)\n",
    "print(\"BOW 特征矩阵形状：\", X_train_bow.shape)\n",
    "\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train_bow, y_train_raw)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "acc_bow = accuracy_score(y_test_raw, y_pred_bow)\n",
    "print(\"BOW + LogisticRegression 测试集准确率: {:.4f}\".format(acc_bow))\n",
    "\n",
    "print(\"\\n=== TF-IDF 模型 ===\")\n",
    "vectorizer_tfidf = TfidfVectorizer(\n",
    "    max_features=MAX_FEATURES,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(train_texts)\n",
    "X_test_tfidf  = vectorizer_tfidf.transform(test_texts)\n",
    "print(\"TF-IDF 特征矩阵形状：\", X_train_tfidf.shape)\n",
    "\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train_raw)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "acc_tfidf = accuracy_score(y_test_raw, y_pred_tfidf)\n",
    "print(\"TF-IDF + LogisticRegression 测试集准确率: {:.4f}\".format(acc_tfidf))\n",
    "\n",
    "\n",
    "print(\"\\n=== Embedding 神经网络 ===\")\n",
    "maxlen_embed = MAXLEN_DEFAULT\n",
    "embedding_dim_embed = 32\n",
    "\n",
    "x_train_embed = sequence.pad_sequences(x_train_raw, maxlen=maxlen_embed)\n",
    "x_test_embed  = sequence.pad_sequences(x_test_raw,  maxlen=maxlen_embed)\n",
    "\n",
    "model_embed = build_embedding_model(maxlen_embed, embedding_dim_embed)\n",
    "model_embed.summary()\n",
    "\n",
    "history_embed = model_embed.fit(\n",
    "    x_train_embed, y_train_raw,\n",
    "    epochs=EPOCHS_EMBED,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "test_loss_embed, test_acc_embed = model_embed.evaluate(x_test_embed, y_test_raw, verbose=0)\n",
    "print(\"Embedding + NN 测试集准确率: {:.4f}\".format(test_acc_embed))\n",
    "\n",
    "\n",
    "results_compare = {\n",
    "    \"bow_logreg\": float(acc_bow),\n",
    "    \"tfidf_logreg\": float(acc_tfidf),\n",
    "    \"embedding_nn\": float(test_acc_embed)\n",
    "}\n",
    "print(\"\\n=== 结果汇总 ===\")\n",
    "print(results_compare)\n",
    "\n",
    "plot_history(history_embed, title_prefix=\"[Embedding vs BOW/TF-IDF] 选定参数\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验 1 小结（请在此写上你自己的结论）\n",
    "\n",
    "你可以围绕以下几点写总结（示例）：\n",
    "\n",
    "- 在不同 **maxlen** 与 **embedding_dim** 组合下，模型的验证/测试准确率如何变化？  \n",
    "  - 例如：序列过短是否会丢失关键信息？序列过长是否带来过拟合或训练变慢？  \n",
    "- 对比 **One-Hot(BOW)**、**TF-IDF** 与 **Embedding 网络** 的性能：  \n",
    "  - 哪种方法效果最好？  \n",
    "  - 传统的 BOW / TF-IDF 与神经网络 Embedding 各自的优缺点？  \n",
    "- 结合 PPT 中对 **文本向量化方法** 的介绍，谈谈你对词嵌入优势的理解。fileciteturn1file0\n",
    "\n",
    "> 请在这里用自然语言写 5~10 句自己的分析，而不是简单贴数字。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM / 堆叠 LSTM / BiLSTM 电影评论分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 MAXLEN_RNN 重新做 padding\n",
    "x_train_rnn = sequence.pad_sequences(x_train_raw, maxlen=MAXLEN_RNN)\n",
    "x_test_rnn  = sequence.pad_sequences(x_test_raw,  maxlen=MAXLEN_RNN)\n",
    "\n",
    "print(\"x_train_rnn shape:\", x_train_rnn.shape)\n",
    "print(\"x_test_rnn shape:\", x_test_rnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBED_DIM_RNN, input_length=MAXLEN_RNN))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_stacked_lstm_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBED_DIM_RNN, input_length=MAXLEN_RNN))\n",
    "    model.add(LSTM(32, dropout=0.1, return_sequences=True))\n",
    "    model.add(LSTM(32, dropout=0.1, return_sequences=True))\n",
    "    model.add(LSTM(32, dropout=0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_bilstm_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(MAX_FEATURES, EMBED_DIM_RNN, input_length=MAXLEN_RNN))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results = {}\n",
    "\n",
    "print(\"\\n=== 单层 LSTM ===\")\n",
    "model_lstm = build_lstm_model()\n",
    "model_lstm.summary()\n",
    "history_lstm = model_lstm.fit(\n",
    "    x_train_rnn, y_train_raw,\n",
    "    epochs=EPOCHS_RNN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "test_loss_lstm, test_acc_lstm = model_lstm.evaluate(x_test_rnn, y_test_raw, verbose=0)\n",
    "rnn_results[\"lstm\"] = float(test_acc_lstm)\n",
    "print(\"LSTM 测试集准确率: {:.4f}\".format(test_acc_lstm))\n",
    "plot_history(history_lstm, title_prefix=\"[RNN] 单层 LSTM\")\n",
    "\n",
    "print(\"\\n=== 堆叠 LSTM ===\")\n",
    "model_stacked = build_stacked_lstm_model()\n",
    "model_stacked.summary()\n",
    "history_stacked = model_stacked.fit(\n",
    "    x_train_rnn, y_train_raw,\n",
    "    epochs=EPOCHS_RNN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "test_loss_stacked, test_acc_stacked = model_stacked.evaluate(x_test_rnn, y_test_raw, verbose=0)\n",
    "rnn_results[\"stacked_lstm\"] = float(test_acc_stacked)\n",
    "print(\"堆叠 LSTM 测试集准确率: {:.4f}\".format(test_acc_stacked))\n",
    "plot_history(history_stacked, title_prefix=\"[RNN] 堆叠 LSTM\")\n",
    "\n",
    "\n",
    "print(\"\\n=== BiLSTM ===\")\n",
    "model_bilstm = build_bilstm_model()\n",
    "model_bilstm.summary()\n",
    "history_bilstm = model_bilstm.fit(\n",
    "    x_train_rnn, y_train_raw,\n",
    "    epochs=EPOCHS_RNN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "test_loss_bilstm, test_acc_bilstm = model_bilstm.evaluate(x_test_rnn, y_test_raw, verbose=0)\n",
    "rnn_results[\"bilstm\"] = float(test_acc_bilstm)\n",
    "print(\"BiLSTM 测试集准确率: {:.4f}\".format(test_acc_bilstm))\n",
    "plot_history(history_bilstm, title_prefix=\"[RNN] BiLSTM\")\n",
    "\n",
    "print(\"\\n=== RNN 模型结果汇总 ===\")\n",
    "print(rnn_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验 2 小结（请在此写上你自己的结论）\n",
    "\n",
    "你可以围绕以下问题进行总结（示例）：\n",
    "\n",
    "- 单层 LSTM、堆叠 LSTM、BiLSTM 三种结构在验证集 / 测试集上的准确率分别是多少？  \n",
    "- 堆叠 LSTM 是否一定比单层 LSTM 好？在本实验中的结果如何？  \n",
    "- BiLSTM 在本任务上是否带来了明显收益？结合 PPT 中对 **双向 RNN / 堆叠 RNN** 的解释谈谈你的理解。fileciteturn1file0  \n",
    "- 模型深度、双向结构会带来怎样的计算代价？在小数据任务中是否总是值得？\n",
    "\n",
    "> 在完成实验后，请写出你自己的分析和感想（5~10 句即可），将其作为作业报告的一部分。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
